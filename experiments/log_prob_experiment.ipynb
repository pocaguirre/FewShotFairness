{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "sys.path.append(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import logging\n",
    "\n",
    "from typing import Iterable, List, Dict, Any, Tuple\n",
    "\n",
    "import backoff\n",
    "\n",
    "import openai\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from src.models.apimodel import APIModel\n",
    "from src.demonstrations import *\n",
    "\n",
    "logger = logging.getLogger(__name__ + \".models\")\n",
    "logging.getLogger(\"openai\").setLevel(logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_demonstration(\n",
    "    demonstration_name: str,\n",
    "    demonstration_params: Dict[str, Any],\n",
    "    train_df: pd.DataFrame,\n",
    "    test_df: pd.DataFrame,\n",
    "    overall_demographics: List[str],\n",
    ") -> Tuple[List[str], pd.DataFrame, str]:\n",
    "    \"\"\"Build demonstrations based on parameters\n",
    "\n",
    "    :param demonstration_name: name of demonstration\n",
    "    :type demonstration_name: str\n",
    "    :param demonstration_params: parameters for demonstration\n",
    "    :type demonstration_params: Dict[str, Any]\n",
    "    :param train_df: train prompts and demographics\n",
    "    :type train_df: pd.DataFrame\n",
    "    :param test_df: train prompts and demographics\n",
    "    :type test_df: pd.DataFrame\n",
    "    :param overall_demographics: demographics to focus on\n",
    "    :type overall_demographics: List[str]\n",
    "    :raises ValueError: demonstration does not exist\n",
    "    :return: the list of formed demonstrations\n",
    "    :rtype: List[str]\n",
    "    \"\"\"\n",
    "\n",
    "    demonstrations = {\n",
    "        \"excluding\": ExcludingDemographic,\n",
    "        \"zeroshot\": RandomSampler,\n",
    "        \"random\": RandomSampler,\n",
    "        \"stratified\": StratifiedSampler,\n",
    "        \"within\": WithinDemographic,\n",
    "        \"similarity\": SimilarityDemonstration,\n",
    "        \"diversity\": DiversityDemonstration\n",
    "    }\n",
    "\n",
    "    shots = None\n",
    "\n",
    "    if demonstration_name == \"zeroshot\":\n",
    "        shots = 0\n",
    "        demonstration_params[\"shots\"] = 0\n",
    "    else:\n",
    "        shots = demonstration_params[\"shots\"]\n",
    "\n",
    "    try:\n",
    "        demonstration = demonstrations[demonstration_name]\n",
    "    except KeyError:\n",
    "        raise ValueError(f\"{demonstration_name} does not exist!\")\n",
    "\n",
    "    sampler = demonstration(shots=shots)\n",
    "\n",
    "    prompts, filtered_test_df = sampler.create_demonstrations(train_df, test_df, overall_demographics)\n",
    "\n",
    "    return prompts, filtered_test_df, sampler.type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT(APIModel):\n",
    "    \"\"\"Code modified from\n",
    "    https://github.com/isabelcachola/generative-prompting/blob/main/genprompt/models.py\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model_name: str, temperature: float = 1, max_tokens: int = 5):\n",
    "\n",
    "        super().__init__(model_name, temperature, max_tokens)\n",
    "\n",
    "        openai.api_key = os.environ[\"OPENAI_API_KEY\"]\n",
    "        self.batch_size = 20\n",
    "\n",
    "    @backoff.on_exception(\n",
    "        backoff.expo,\n",
    "        (\n",
    "            openai.error.RateLimitError,\n",
    "            openai.error.APIError,\n",
    "            openai.error.Timeout,\n",
    "            openai.error.ServiceUnavailableError,\n",
    "        ),\n",
    "    )\n",
    "    def get_response(self, prompt: Iterable[str]) -> Dict[str, Any]:\n",
    "        \"\"\"Overloaded get_response to deal with batching\n",
    "\n",
    "        :param prompt: prompts as batch\n",
    "        :type prompt: Iterable[str]\n",
    "        :return: responses from GPT3 API endpoint\n",
    "        :rtype: Dict[str, Any]\n",
    "        \"\"\"\n",
    "        response = openai.Completion.create(\n",
    "            model=self.model_name,\n",
    "            prompt=prompt,\n",
    "            temperature=self.temperature,\n",
    "            max_tokens=self.max_tokens,\n",
    "            logprobs=5\n",
    "        )\n",
    "\n",
    "        return response\n",
    "\n",
    "    def format_response(self, response: Dict[str, Any]) -> Tuple[str, Dict[str, float]]:\n",
    "        text = response[\"text\"].replace(\"\\n\", \" \").strip()\n",
    "        top_logprobs = response[\"logprobs\"][\"top_logprobs\"]\n",
    "\n",
    "        output = (text, top_logprobs)\n",
    "\n",
    "        return output\n",
    "\n",
    "    def generate_from_prompts(self, examples: Iterable[str]) -> List[str]:\n",
    "        lines_length = len(examples)\n",
    "        logger.info(f\"Num examples = {lines_length}\")\n",
    "        i = 0\n",
    "\n",
    "        responses = []\n",
    "\n",
    "        for i in tqdm(range(0, lines_length, self.batch_size), ncols=0):\n",
    "\n",
    "            # batch prompts together\n",
    "            prompt_batch = examples[i : min(i + self.batch_size, lines_length)]\n",
    "            try:\n",
    "                # try to get respones\n",
    "                response = self.get_response(prompt_batch)\n",
    "\n",
    "                print(response)\n",
    "\n",
    "                response_batch = [\"\"] * len(prompt_batch)\n",
    "\n",
    "                # order the responses as they are async\n",
    "                for choice in response.choices:\n",
    "                    response_batch[choice.index] = self.format_response(choice.text)\n",
    "\n",
    "                responses.extend(response_batch)\n",
    "\n",
    "            # catch any connection exceptions\n",
    "            except:\n",
    "\n",
    "                # try each prompt individually\n",
    "                for i in range(len(prompt_batch)):\n",
    "                    try:\n",
    "                        _r = self.get_response(prompt_batch[i])[\"choices\"][0]\n",
    "                        line = self.format_response(_r)\n",
    "                        responses.append(line)\n",
    "                    except:\n",
    "                        # if there is an exception make blank\n",
    "                        l_prompt = len(prompt_batch[i])\n",
    "                        _r = self.get_response(prompt_batch[i][l_prompt - 2000 :])[\n",
    "                            \"choices\"\n",
    "                        ][0]\n",
    "                        line = self.format_response(_r)\n",
    "                        responses.append(line)\n",
    "\n",
    "        return responses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.datasets import HateXplainRace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hate = HateXplainRace('../data/HateXplain')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df, overall_demographics = hate.create_prompts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt = GPT(\"text-davinci-003\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demonstrations = [\"within\", \"similarity\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = []\n",
    "for demonstration in demonstrations:\n",
    "    prompts, filtered_test_df, sampler_type = build_demonstration(demonstration, {\"shots\" : 5}, train_df, test_df, overall_demographics)\n",
    "    \n",
    "    responses = gpt.generate_from_prompts(prompts)\n",
    "\n",
    "    outputs.append(responses)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_responses = [i[0] for i in outputs[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_clean = copy.deepcopy(text_responses)\n",
    "\n",
    "    # clean up predictions\n",
    "preds_clean = [x.lower() for x in preds_clean]\n",
    "\n",
    "conv = lambda i: i or \"\"\n",
    "preds_clean = [conv(i) for i in preds_clean]\n",
    "\n",
    "# create list of all labels\n",
    "labels_set = list(set(test_df[\"labels\"].tolist()))\n",
    "\n",
    "# map labels to numbers to make it easier for sklearn calculations\n",
    "labels_dict = dict(zip(labels_set, range(len(labels_set))))\n",
    "\n",
    "# map the labels lists to dummy labels\n",
    "dummy_labels = [labels_dict[x] for x in test_df[\"labels\"].tolist()]\n",
    "\n",
    "dummy_preds = []\n",
    "\n",
    "for pred in preds_clean:\n",
    "\n",
    "    # see if any of the labels are in the response\n",
    "    for label in labels_set:\n",
    "        if pred.find(label) != -1:\n",
    "            dummy_preds.append(labels_dict[label])\n",
    "            break\n",
    "        # if not we add -1 instead\n",
    "    else:\n",
    "        dummy_preds.append(-1)\n",
    "\n",
    "dummy_preds = np.array(dummy_preds)\n",
    "dummy_labels = np.array(dummy_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0,   1,   2,   3,   4,   5,   6,   8,   9,  11,  13,  14,  15,\n",
       "        16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  28,  29,  30,\n",
       "        31,  32,  35,  36,  37,  38,  39,  40,  41,  43,  44,  46,  48,\n",
       "        50,  51,  52,  54,  55,  56,  57,  59,  60,  61,  62,  63,  64,\n",
       "        65,  66,  67,  69,  70,  71,  72,  73,  74,  75,  77,  78,  79,\n",
       "        80,  83,  84,  85,  86,  87,  90,  91,  92,  93,  94,  97,  98,\n",
       "        99, 100, 101, 102, 103, 105, 106, 107, 108, 109, 110, 111, 112,\n",
       "       113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,\n",
       "       127, 129, 130, 132, 134, 135, 136, 137, 139, 140, 141, 142, 143,\n",
       "       144, 146, 147, 148, 149, 150, 151, 152, 154, 156, 158, 159, 160,\n",
       "       161, 162, 164, 167, 168, 169, 170, 171, 176, 177, 178, 179, 181,\n",
       "       182, 185, 186, 187, 188, 189, 191, 192, 193, 195, 197, 199, 200,\n",
       "       201, 202, 203, 204, 205, 207, 208, 209, 210, 211, 213, 215, 218,\n",
       "       219, 222, 223, 225, 228, 229, 230, 231, 232, 233, 235, 238, 239,\n",
       "       240, 242, 243, 244, 246, 248, 249, 252, 253, 254, 255, 256, 257,\n",
       "       259, 260, 261, 263, 264, 265, 266, 268, 269, 270, 271, 272, 274,\n",
       "       275, 276, 278, 279, 281, 282, 283, 284, 285, 286, 287, 288, 289,\n",
       "       291, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304,\n",
       "       305, 306, 307, 308, 309, 310, 314, 315, 316, 318, 319, 320, 321,\n",
       "       322, 324, 325, 326, 327, 328, 330, 331, 334, 335, 336, 338, 339,\n",
       "       341, 342, 343, 344, 345, 347, 348, 349, 350, 352, 354, 358, 360,\n",
       "       361, 362, 363, 364, 365, 368, 369, 370, 371, 373, 377, 378, 379,\n",
       "       380, 381, 382, 383, 384, 386, 387, 388, 389, 390, 391, 392, 393,\n",
       "       394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406,\n",
       "       407, 408, 409, 412, 413, 415, 416, 417, 418, 419, 420, 423, 424,\n",
       "       425, 427, 429, 430, 431, 432, 434, 435, 436, 437, 438, 440, 441,\n",
       "       442, 444, 445, 446, 447, 449, 452, 453, 454, 455, 456, 457, 459,\n",
       "       460, 461, 462, 464, 466, 467, 468, 469, 470, 471, 473, 474, 475,\n",
       "       478, 480, 481, 482, 483, 485, 486, 487, 488, 489, 490, 491, 492,\n",
       "       493, 495, 497, 499, 500, 501, 502, 503, 505, 506, 508, 509, 510,\n",
       "       511, 514, 515, 516, 517, 518, 519, 520, 522, 523, 524, 525, 526,\n",
       "       527, 528, 529, 530, 531, 532, 533, 534, 535, 536, 540, 541, 543,\n",
       "       544, 545, 546, 547, 548, 550, 551, 552, 553, 554, 555, 557, 558,\n",
       "       559, 560, 561, 562, 565, 566, 567, 568, 569, 570, 571, 572, 573,\n",
       "       575, 577, 580, 581, 582, 583, 584, 585, 586, 587, 588, 589, 592,\n",
       "       593, 594, 596, 597, 598, 599, 600, 601, 603, 604, 605, 608, 609,\n",
       "       610, 611, 612, 613, 615, 616, 618, 621, 623, 624, 625, 626, 627,\n",
       "       628, 629, 630, 631, 633, 634, 635, 637, 638, 640, 641, 642, 643,\n",
       "       644, 645, 646, 647, 648, 650, 651, 652, 653, 654, 655, 656, 657,\n",
       "       660, 662, 663, 664, 665, 666, 667, 668, 670, 671, 672, 673, 679,\n",
       "       680])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(dummy_preds == dummy_labels).nonzero()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance = metrics(\n",
    "    text_responses,\n",
    "    test_df[\"labels\"].tolist(),\n",
    "    \"hatexplain-race\",\n",
    "    test_df[\"demographics\"].tolist(),\n",
    "    overall_demographics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'recall': {'African': array([0.36842105, 0.84591195]),\n",
       "  'Arab': array([0.69230769, 0.85185185]),\n",
       "  'Asian': array([0.57142857, 0.9       ]),\n",
       "  'Hispanic': array([0.5       , 0.83870968]),\n",
       "  'Caucasian': array([0.48      , 0.76785714])},\n",
       " 'specificity': {'African': array([0.84591195, 0.36842105]),\n",
       "  'Arab': array([0.85185185, 0.69230769]),\n",
       "  'Asian': array([0.9       , 0.57142857]),\n",
       "  'Hispanic': array([0.83870968, 0.5       ]),\n",
       "  'Caucasian': array([0.76785714, 0.48      ])},\n",
       " 'score': {'African': 0.5971360321532843,\n",
       "  'Arab': 0.6878224974200207,\n",
       "  'Asian': 0.7485714285714287,\n",
       "  'Hispanic': 0.6224961479198767,\n",
       "  'Caucasian': 0.6198620689655172},\n",
       " 'total_score': 0.6477343265052762,\n",
       " 'max_gaps': {'no': ['Arab',\n",
       "   'African',\n",
       "   0.32388663967611336,\n",
       "   0.6761133603238867],\n",
       "  'yes': ['Asian', 'Caucasian', 0.13214285714285712, 0.8678571428571429]}}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(outputs, open( \"logproboutputs.p\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(train_df, open(\"traindflogprobs.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(test_df, open(\"testdflogprobs.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fairness",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
