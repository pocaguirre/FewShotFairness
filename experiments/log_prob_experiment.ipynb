{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "sys.path.append(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import math\n",
    "\n",
    "import copy\n",
    "\n",
    "import collections\n",
    "\n",
    "import logging\n",
    "\n",
    "from typing import Iterable, List, Dict, Any, Tuple\n",
    "\n",
    "import backoff\n",
    "\n",
    "import openai\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from src.models.apimodel import APIModel\n",
    "\n",
    "from src.demonstrations import *\n",
    "\n",
    "from src.datasets import HateXplainRace\n",
    "\n",
    "from src.utils import metrics\n",
    "\n",
    "from src.__main__  import build_demonstration\n",
    "\n",
    "\n",
    "logger = logging.getLogger(__name__ + \".models\")\n",
    "logging.getLogger(\"openai\").setLevel(logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_dicts(*dicts):\n",
    "    res = collections.defaultdict(list)\n",
    "    for d in dicts:\n",
    "        for k, v in d.items():\n",
    "            res[k].append(v)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTLogProb(APIModel):\n",
    "    \"\"\"Code modified from\n",
    "    https://github.com/isabelcachola/generative-prompting/blob/main/genprompt/models.py\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model_name: str, temperature: float = 1, max_tokens: int = 5):\n",
    "\n",
    "        super().__init__(model_name, temperature, max_tokens)\n",
    "\n",
    "        openai.api_key = os.environ[\"OPENAI_API_KEY\"]\n",
    "        self.batch_size = 20\n",
    "\n",
    "    @backoff.on_exception(\n",
    "        backoff.expo,\n",
    "        (\n",
    "            openai.error.RateLimitError,\n",
    "            openai.error.APIError,\n",
    "            openai.error.Timeout,\n",
    "            openai.error.ServiceUnavailableError,\n",
    "        ),\n",
    "    )\n",
    "    def get_response(self, prompt: Iterable[str]) -> Dict[str, Any]:\n",
    "        \"\"\"Overloaded get_response to deal with batching\n",
    "\n",
    "        :param prompt: prompts as batch\n",
    "        :type prompt: Iterable[str]\n",
    "        :return: responses from GPT3 API endpoint\n",
    "        :rtype: Dict[str, Any]\n",
    "        \"\"\"\n",
    "        response = openai.Completion.create(\n",
    "            model=self.model_name,\n",
    "            prompt=prompt,\n",
    "            temperature=self.temperature,\n",
    "            max_tokens=self.max_tokens,\n",
    "            logprobs=5\n",
    "        )\n",
    "\n",
    "        return response\n",
    "\n",
    "    def format_response(self, response: Dict[str, Any]) -> Tuple[str, Dict[str, float]]:\n",
    "        text = response[\"text\"].replace(\"\\n\", \" \").strip()\n",
    "        top_logprobs = response[\"logprobs\"][\"top_logprobs\"]\n",
    "\n",
    "        output = (text, top_logprobs)\n",
    "\n",
    "        return output\n",
    "\n",
    "    def generate_from_prompts(self, examples: Iterable[str]) -> List[str]:\n",
    "        lines_length = len(examples)\n",
    "        logger.info(f\"Num examples = {lines_length}\")\n",
    "        i = 0\n",
    "\n",
    "        responses = []\n",
    "\n",
    "        for i in tqdm(range(0, lines_length, self.batch_size), ncols=0):\n",
    "\n",
    "            # batch prompts together\n",
    "            prompt_batch = examples[i : min(i + self.batch_size, lines_length)]\n",
    "            try:\n",
    "                # try to get respones\n",
    "                response = self.get_response(prompt_batch)\n",
    "\n",
    "                print(response)\n",
    "\n",
    "                response_batch = [\"\"] * len(prompt_batch)\n",
    "\n",
    "                # order the responses as they are async\n",
    "                for choice in response.choices:\n",
    "                    response_batch[choice.index] = self.format_response(choice.text)\n",
    "\n",
    "                responses.extend(response_batch)\n",
    "\n",
    "            # catch any connection exceptions\n",
    "            except:\n",
    "\n",
    "                # try each prompt individually\n",
    "                for i in range(len(prompt_batch)):\n",
    "                    try:\n",
    "                        _r = self.get_response(prompt_batch[i])[\"choices\"][0]\n",
    "                        line = self.format_response(_r)\n",
    "                        responses.append(line)\n",
    "                    except:\n",
    "                        # if there is an exception make blank\n",
    "                        l_prompt = len(prompt_batch[i])\n",
    "                        _r = self.get_response(prompt_batch[i][l_prompt - 2000 :])[\n",
    "                            \"choices\"\n",
    "                        ][0]\n",
    "                        line = self.format_response(_r)\n",
    "                        responses.append(line)\n",
    "\n",
    "        return responses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hate = HateXplainRace('../data/HateXplain')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df, overall_demographics = hate.create_prompts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt = GPTLogProb(\"text-davinci-003\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demonstrations = [\"within\", \"similarity\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = []\n",
    "\n",
    "labels = test_df[\"labels\"].tolist()\n",
    "\n",
    "for demonstration in demonstrations:\n",
    "    prompts, filtered_test_df, sampler_type = build_demonstration(demonstration, {\"shots\" : 5}, train_df, test_df, overall_demographics)\n",
    "    \n",
    "    responses = gpt.generate_from_prompts(prompts)\n",
    "\n",
    "    text_responses = [i[0] for i in responses]\n",
    "\n",
    "    preds_clean = copy.deepcopy(text_responses)\n",
    "\n",
    "    # clean up predictions\n",
    "    preds_clean = [x.lower() for x in preds_clean]\n",
    "\n",
    "    conv = lambda i: i or \"\"\n",
    "    preds_clean = [conv(i) for i in preds_clean]\n",
    "\n",
    "    # create list of all labels\n",
    "    labels_set = list(set(test_df[\"labels\"].tolist()))\n",
    "\n",
    "    # map labels to numbers to make it easier for sklearn calculations\n",
    "    labels_dict = dict(zip(labels_set, range(len(labels_set))))\n",
    "\n",
    "    # map the labels lists to dummy labels\n",
    "    dummy_labels = [labels_dict[x] for x in test_df[\"labels\"].tolist()]\n",
    "\n",
    "    dummy_preds = []\n",
    "\n",
    "    for pred in preds_clean:\n",
    "\n",
    "        # see if any of the labels are in the response\n",
    "        for label in labels_set:\n",
    "            if pred.find(label) != -1:\n",
    "                dummy_preds.append(labels_dict[label])\n",
    "                break\n",
    "            # if not we add -1 instead\n",
    "        else:\n",
    "            dummy_preds.append(-1)\n",
    "\n",
    "    dummy_preds = np.array(dummy_preds)\n",
    "    dummy_labels = np.array(dummy_labels)\n",
    "\n",
    "    incorrect = (dummy_preds != dummy_labels).nonzero()[0]\n",
    "\n",
    "    responses_incorrect = [(text_responses[i], labels[i]) for i in incorrect]\n",
    "\n",
    "    total = 0\n",
    "\n",
    "    differences = []\n",
    "\n",
    "    for i in range(len(responses_incorrect)):\n",
    "        response = responses_incorrect[i]\n",
    "\n",
    "        label = response[1]\n",
    "\n",
    "        response_openai = responses_incorrect[i][0][1]\n",
    "\n",
    "        response_dict = dict()\n",
    "\n",
    "        response_dict = merge_dicts(*response_openai)\n",
    "        \n",
    "        response_dict = {k: max(v) for (k,v) in response_dict.items()}\n",
    "\n",
    "        contains_label = [x for x in list(response_dict.keys()) if label in x.lower()]\n",
    "\n",
    "        if len(contains_label) != 0:\n",
    "\n",
    "            total+=1\n",
    "\n",
    "            best_log_prob_label = math.e**max([response_dict[x] for x in contains_label])\n",
    "\n",
    "            pred_log_prob = math.e**max(response_dict.values())\n",
    "\n",
    "\n",
    "            differences.append(best_log_prob_label-pred_log_prob)\n",
    "    \n",
    "    print(total)\n",
    "    print(sum(differences)/len(differences))\n",
    "    print(total/len(responses_incorrect))\n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fairness",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
