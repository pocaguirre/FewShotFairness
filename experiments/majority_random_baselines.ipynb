{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.datasets import BiasInBios\n",
    "from src.datasets import HateXplainRace\n",
    "from src.datasets import HateXplainGender\n",
    "from src.datasets import TwitterAAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_output(baseline, performance): \n",
    "    result = [\n",
    "            baseline,\n",
    "            'N/A',\n",
    "            'N/A',\n",
    "            'N/A',\n",
    "            performance[\"total_score\"],\n",
    "    ]\n",
    "\n",
    "    group_results = performance[\"score\"]\n",
    "\n",
    "    gaps = performance[\"max_gaps\"]\n",
    "\n",
    "    for group_result in group_results:\n",
    "        result.append({group_result: group_results[group_result]})\n",
    "\n",
    "    gaps = dict(sorted(gaps.items(), key=lambda item: item[1][3]))\n",
    "\n",
    "    result.append(list(gaps.values())[0][3])\n",
    "\n",
    "    for class_name in gaps:\n",
    "        gap = gaps[class_name]\n",
    "\n",
    "        result.append({class_name: list(gap)})\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "bias = BiasInBios(\"../data/biasbios\")\n",
    "haterace = HateXplainRace(\"../data/HateXplain\")\n",
    "hategender = HateXplainGender(\"../data/HateXplain\")\n",
    "aae = TwitterAAE(\"../data/moji/twitteraae_sentiment_race\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "bias_train_df, bias_test_df, bias_demo = bias.create_prompts()\n",
    "haterace_train_df, haterace_test_df, haterace_demo = haterace.create_prompts()\n",
    "hategender_train_df, hategender_test_df, hategender_demo = hategender.create_prompts()\n",
    "aae_train_df, aae_test_df, aae_demo = aae.create_prompts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Majority"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "professor       76771\n",
       "physician       25067\n",
       "attorney        21194\n",
       "photographer    15810\n",
       "journalist      12967\n",
       "psychologist    11891\n",
       "teacher         10527\n",
       "dentist          9411\n",
       "Name: labels, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# bias in bios\n",
    "bias_train_df.labels.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "majority_baseline_bias = len(bias_test_df) * ['professor']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['majority',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 0.027777777777777776,\n",
       " {'m': 0.027777777777777776},\n",
       " {'f': 0.027777777777777776},\n",
       " 1.0,\n",
       " {'attorney': ['m', 'm', 0.0, 1.0]},\n",
       " {'dentist': ['m', 'm', 0.0, 1.0]},\n",
       " {'teacher': ['m', 'm', 0.0, 1.0]},\n",
       " {'photographer': ['m', 'm', 0.0, 1.0]},\n",
       " {'physician': ['m', 'm', 0.0, 1.0]},\n",
       " {'journalist': ['m', 'm', 0.0, 1.0]},\n",
       " {'psychologist': ['m', 'm', 0.0, 1.0]},\n",
       " {'professor': ['m', 'm', 0.0, 1.0]}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parse_output('majority', metrics(majority_baseline_bias, bias_test_df['labels'].tolist(), 'bias', bias_test_df['demographics'].tolist(), bias.demographics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "happy    80000\n",
       "sad      80000\n",
       "Name: labels, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# aae\n",
    "aae_train_df.labels.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "majority_baseline_aae = len(aae_test_df) * ['sad']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['majority',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 0.3333333333333333,\n",
       " {'aa': 0.3333333333333333},\n",
       " {'wh': 0.3333333333333333},\n",
       " 1.0,\n",
       " {'sad': ['aa', 'aa', 0.0, 1.0]},\n",
       " {'happy': ['aa', 'aa', 0.0, 1.0]}]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parse_output('majority', metrics(majority_baseline_aae, aae_test_df['labels'].tolist(), 'aae', aae_test_df['demographics'].tolist(), aae.demographics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "yes    4371\n",
       "no     1005\n",
       "Name: labels, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# hatexplain race\n",
    "haterace_train_df.labels.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "majority_baseline_haterace = len(haterace_test_df) * ['yes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['majority',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 0.44362745098039214,\n",
       " {'African': 0.4588744588744589},\n",
       " {'Arab': 0.47161572052401746},\n",
       " {'Asian': 0.40540540540540543},\n",
       " {'Hispanic': 0.46969696969696967},\n",
       " {'Caucasian': 0.34567901234567905},\n",
       " 1.0,\n",
       " {'yes': ['African', 'African', 0.0, 1.0]},\n",
       " {'no': ['African', 'African', 0.0, 1.0]}]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parse_output('majority', metrics(majority_baseline_haterace, haterace_test_df['labels'].tolist(), 'hatexplain-race', haterace_test_df['demographics'].tolist(), haterace.demographics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "yes    1937\n",
       "no      669\n",
       "Name: labels, dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# hatexplain gender\n",
    "hategender_train_df.labels.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "majority_baseline_hategender = len(hategender_test_df) * ['yes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['majority',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 0.42985611510791366,\n",
       " {'Men': 0.379746835443038},\n",
       " {'Women': 0.4381551362683438},\n",
       " 1.0,\n",
       " {'yes': ['Men', 'Men', 0.0, 1.0]},\n",
       " {'no': ['Men', 'Men', 0.0, 1.0]}]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parse_output('majority', metrics(majority_baseline_hategender, hategender_test_df['labels'].tolist(), 'hatexplain-race', hategender_test_df['demographics'].tolist(), hategender.demographics))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bias\n",
    "random_baseline_bias = random.choices(bias.labels, k=len(bias_test_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['random',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 0.1187832435932761,\n",
       " {'m': 0.11610075900028297},\n",
       " {'f': 0.12161981367397924},\n",
       " 0.98,\n",
       " {'photographer': ['f', 'm', 0.020000000000000004, 0.98]},\n",
       " {'professor': ['m', 'f', 0.01999999999999999, 0.98]},\n",
       " {'psychologist': ['f', 'm', 0.018000000000000002, 0.982]},\n",
       " {'journalist': ['f', 'm', 0.011999999999999997, 0.988]},\n",
       " {'dentist': ['f', 'm', 0.010000000000000009, 0.99]},\n",
       " {'attorney': ['f', 'm', 0.008000000000000007, 0.992]},\n",
       " {'physician': ['m', 'f', 0.006000000000000005, 0.994]},\n",
       " {'teacher': ['f', 'm', 0.0, 1.0]}]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parse_output('random', metrics(random_baseline_bias, bias_test_df['labels'].tolist(), 'bias', bias_test_df['demographics'].tolist(), bias.demographics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aae\n",
    "random_baseline_aae = random.choices(aae.labels, k=len(aae_test_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['random',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 0.49562366813124864,\n",
       " {'aa': 0.4982484633859191},\n",
       " {'wh': 0.4929988592474333},\n",
       " 0.9944999999999999,\n",
       " {'happy': ['aa', 'wh', 0.005500000000000005, 0.9944999999999999]},\n",
       " {'sad': ['aa', 'wh', 0.0050000000000000044, 0.995]}]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parse_output('random', metrics(random_baseline_aae, aae_test_df['labels'].tolist(), 'aae', aae_test_df['demographics'].tolist(), aae.demographics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hatexplain race\n",
    "random_baseline_haterace = random.choices(haterace.labels, k=len(haterace_test_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['random',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 0.4305019712014842,\n",
       " {'African': 0.40324257893741733},\n",
       " {'Arab': 0.4053582752060875},\n",
       " {'Asian': 0.5580357142857143},\n",
       " {'Hispanic': 0.4493006993006993},\n",
       " {'Caucasian': 0.45835948005378757},\n",
       " 0.4,\n",
       " {'no': ['Hispanic', 'Caucasian', 0.6, 0.4]},\n",
       " {'yes': ['Asian', 'Hispanic', 0.21397849462365587, 0.7860215053763442]}]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parse_output('random', metrics(random_baseline_haterace, haterace_test_df['labels'].tolist(), 'hatexplain-race', haterace_test_df['demographics'].tolist(), haterace.demographics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hatexplain gender\n",
    "random_baseline_hategender = random.choices(hategender.labels, k=len(hategender_test_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['random',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 0.4502115059221658,\n",
       " {'Men': 0.5416666666666666},\n",
       " {'Women': 0.43135496981650834},\n",
       " 0.9022328548644338,\n",
       " {'yes': ['Men', 'Women', 0.09776714513556617, 0.9022328548644338]},\n",
       " {'no': ['Men', 'Women', 0.05173951828724349, 0.9482604817127565]}]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parse_output('random', metrics(random_baseline_hategender, hategender_test_df['labels'].tolist(), 'hatexplain-gender', hategender_test_df['demographics'].tolist(), hategender.demographics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fairness",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
