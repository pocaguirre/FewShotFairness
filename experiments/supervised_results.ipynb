{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Any, Dict\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score, confusion_matrix\n",
    "\n",
    "import copy\n",
    "\n",
    "\n",
    "def metrics(\n",
    "    preds: List[str],\n",
    "    labels: List[str],\n",
    "    demographics: List[List[str]],\n",
    "    overall_demographics: List[str],\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"Returns a dictionary of overall macro f1 score,\n",
    "    recall, specificity, and macro f1 for each group\n",
    "    and largest gap in recall for each label\n",
    "\n",
    "    Code heavily modified from https://github.com/pocaguirre/jiant/blob/master/jiant/ext/fairness/DF_training.py\n",
    "\n",
    "    :param preds: list of predictions from model\n",
    "    :type preds: List[str]\n",
    "    :param labels: list of groundtruth labels\n",
    "    :type labels: List[str]\n",
    "    :param dataset: name of dataset\n",
    "    :type dataset: str\n",
    "    :param demographics: list of lists of demographic groups associated with labels\n",
    "    :type demographics: List[List[str]]\n",
    "    :param overall_demographics: Demographics to focus on in the output\n",
    "    :type overall_demographics: List[str]\n",
    "    :return: dictionary of overall macro f1 score, recall, specificity, and macro f1 for each group\n",
    "    and largest gap in recall for each label\n",
    "    :rtype: Dict[str, Any]\n",
    "    \"\"\"\n",
    "\n",
    "    scores = {\"recall\": {}, \"specificity\": {}, \"score\": {}}\n",
    "\n",
    "    # create subgroups to focus on\n",
    "\n",
    "    # create list of all labels\n",
    "\n",
    "    # map labels to numbers to make it easier for sklearn calculations\n",
    "    label_map = {\"toxic\" : 1, \"non-toxic\" : 0}\n",
    "\n",
    "    # map the labels lists to dummy labels\n",
    "    dummy_labels = [label_map[x] for x in labels]\n",
    "\n",
    "    dummy_preds = np.array(preds)\n",
    "    dummy_labels = np.array(dummy_labels)\n",
    "\n",
    "    # remove predictions that have demographics not in the set\n",
    "    # mostly for hatexplain which has multiple demographics per label\n",
    "    demographic_index = [\n",
    "        i\n",
    "        for i, item in enumerate(demographics)\n",
    "        if len(set(overall_demographics).intersection(set(item))) != 0\n",
    "    ]\n",
    "\n",
    "    demographics_filtered = copy.deepcopy([demographics[i] for i in demographic_index])\n",
    "\n",
    "    dummy_preds = dummy_preds[demographic_index]\n",
    "    dummy_labels = dummy_labels[demographic_index]\n",
    "\n",
    "    print(len(dummy_preds))\n",
    "\n",
    "    # get total score\n",
    "    scores[\"total_score\"] = f1_score(\n",
    "        dummy_labels, dummy_preds, average=\"macro\", labels=list(label_map.values())\n",
    "    )\n",
    "\n",
    "    for dem in overall_demographics:\n",
    "\n",
    "        # filter out items that do not have the specified demographic\n",
    "        index = [i for i, item in enumerate(demographics_filtered) if dem in item]\n",
    "\n",
    "        # calculate f1, recall and specifity for those items\n",
    "        cnf_matrix = confusion_matrix(\n",
    "            dummy_labels[index], dummy_preds[index], labels=list(label_map.values())\n",
    "        )\n",
    "\n",
    "        fn = cnf_matrix.sum(axis=1) - np.diag(cnf_matrix)\n",
    "        tp = np.diag(cnf_matrix)\n",
    "        fp = cnf_matrix.sum(axis=0) - np.diag(cnf_matrix)\n",
    "        tn = cnf_matrix.sum() - (fp + fn + tp)\n",
    "\n",
    "        fn = fn.astype(float)\n",
    "        tp = tp.astype(float)\n",
    "        fp = fp.astype(float)\n",
    "        tn = tn.astype(float)\n",
    "\n",
    "        score = f1_score(\n",
    "            dummy_labels[index],\n",
    "            dummy_preds[index],\n",
    "            average=\"macro\",\n",
    "            labels=list(label_map.values()),\n",
    "        )\n",
    "\n",
    "        recall = tp / (tp + fn)\n",
    "        specificity = tn / (tn + fp)\n",
    "\n",
    "        scores[\"recall\"][dem] = recall\n",
    "        scores[\"specificity\"][dem] = specificity\n",
    "        scores[\"score\"][dem] = score\n",
    "\n",
    "    gaps = []\n",
    "\n",
    "    # calculate all the TPR gaps for every possible combination of demographics\n",
    "    for group1 in scores[\"recall\"]:\n",
    "        for group2 in scores[\"recall\"]:\n",
    "            gap = scores[\"recall\"][group1] - scores[\"recall\"][group2]\n",
    "\n",
    "            gap = np.nan_to_num(gap)\n",
    "\n",
    "            one_minus_gap = 1 - gap\n",
    "            gaps.append([group1, group2, gap, one_minus_gap])\n",
    "\n",
    "    # get the maximum TPR gap per class\n",
    "    max_gaps = dict()\n",
    "    for i, label in enumerate(list(label_map.keys())):\n",
    "        gaps = sorted(gaps, key=lambda x: x[2][i], reverse=True)\n",
    "\n",
    "        max_gaps[label] = copy.deepcopy(gaps[0])\n",
    "        max_gaps[label][2] = max_gaps[label][2][i]\n",
    "        max_gaps[label][3] = max_gaps[label][3][i]\n",
    "\n",
    "    scores[\"max_gaps\"] = max_gaps\n",
    "\n",
    "    return scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/data/caguirre/MultitaskFairness/preds_hatexplain_race_all.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "demographics = [[x] for x in df['race'].tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "bernice_predictions = df['bernice'].tolist()\n",
    "bertweet_predictions = df['bertweet'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_demographics =  [\n",
    "            \"African\",\n",
    "            \"Arab\",\n",
    "            \"Asian\",\n",
    "            \"Hispanic\",\n",
    "            \"Caucasian\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"hatexplain-race\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = df['label'].tolist()\n",
    "\n",
    "label_map = {\"toxic\" : 1, \"non-toxic\" : 0}\n",
    "\n",
    "# map the labels lists to dummy labels\n",
    "dummy_labels = [label_map[x] for x in labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = ['bernice', 'bertweet']\n",
    "predictions = [bernice_predictions, bertweet_predictions]\n",
    "\n",
    "results = []\n",
    "\n",
    "for i in range(len(models)):\n",
    "    model = models[i]\n",
    "    prediction = predictions[i]\n",
    "\n",
    "    performance = metrics(prediction, labels, demographics, overall_demographics)\n",
    "\n",
    "\n",
    "    result = [\n",
    "            model,\n",
    "            'N/A',\n",
    "            'supervised',\n",
    "            performance[\"total_score\"],\n",
    "    ]\n",
    "\n",
    "    group_results = performance[\"score\"]\n",
    "\n",
    "    gaps = performance[\"max_gaps\"]\n",
    "\n",
    "    for group_result in group_results:\n",
    "        result.append({group_result: group_results[group_result]})\n",
    "\n",
    "    gaps = dict(sorted(gaps.items(), key=lambda item: item[1][3]))\n",
    "\n",
    "    result.append(list(gaps.values())[0][3])\n",
    "\n",
    "    for class_name in gaps:\n",
    "\n",
    "        gap = gaps[class_name]\n",
    "\n",
    "        result.append({class_name: list(gap)})\n",
    "\n",
    "    results.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['bernice',\n",
       "  'N/A',\n",
       "  'supervised',\n",
       "  0.6182363667591202,\n",
       "  {'African': 0.6452756073609154},\n",
       "  {'Arab': 0.5564253098499674},\n",
       "  {'Asian': 0.47878787878787876},\n",
       "  {'Hispanic': 0.4615384615384615},\n",
       "  {'Caucasian': 0.512743628185907},\n",
       "  0.5800000000000001,\n",
       "  {'non-toxic': ['Caucasian', 'Hispanic', 0.42, 0.5800000000000001]},\n",
       "  {'toxic': ['Hispanic',\n",
       "    'Caucasian',\n",
       "    0.35663082437275984,\n",
       "    0.6433691756272402]}],\n",
       " ['bertweet',\n",
       "  'N/A',\n",
       "  'supervised',\n",
       "  0.697002997002997,\n",
       "  {'African': 0.7357954545454546},\n",
       "  {'Arab': 0.5202741290691034},\n",
       "  {'Asian': 0.5520833333333334},\n",
       "  {'Hispanic': 0.7321428571428572},\n",
       "  {'Caucasian': 0.6533333333333333},\n",
       "  0.23076923076923084,\n",
       "  {'non-toxic': ['Hispanic', 'Arab', 0.7692307692307692, 0.23076923076923084]},\n",
       "  {'toxic': ['African', 'Asian', 0.26538629419467474, 0.7346137058053253]}]]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fairness",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
