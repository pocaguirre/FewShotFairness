{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "import copy\n",
    "\n",
    "import os\n",
    "\n",
    "from typing import List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "\n",
    "from scipy.stats import kruskal\n",
    "\n",
    "from sklearn.metrics import f1_score, confusion_matrix\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"../output/used_results/\"\n",
    "\n",
    "\n",
    "resamples = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_and_clean_file(file: str) -> Tuple[List[int], List[int], List[int]]:\n",
    "\n",
    "    overall_demographics =  {\"aae\": ['aa', 'wh'], \"hatexplain-race\": [\"African\", \"Arab\", \"Asian\", \"Hispanic\", \"Caucasian\"], \"bias\": [\"m\", \"f\"]}\n",
    "    labels_map = {\"aae\" : {\"happy\": 1, \"sad\": 0, \"fail________\" : -1}, \"hatexplain-race\": {\"yes\" : 1, \"no\" : 0, \"fail________\" : -1}, \"bias\" : {\"attorney\": 0, \"photographer\": 1, \"dentist\" : 2, \"psychologist\" : 3, \"physician\": 4, \"journalist\" : 5, \"teacher\": 6, \"professor\": 7, \"fail________\" : -1}}\n",
    "\n",
    "    results = pd.read_csv(file)\n",
    "\n",
    "    if \"flan\" not in file and \"ul2\" not in file and \"gpt\" not in file:\n",
    "        results['prompt_len'] = results.prompt.str.len()\n",
    "        results['response'] = results.apply(lambda x: x.response[x.prompt_len:], axis=1)\n",
    "\n",
    "    responses = results['response'].fillna(\"\").tolist()\n",
    "    labels = results['label'].tolist()\n",
    "\n",
    "    demographics = [ast.literal_eval(x) for x in results['demographic'].tolist()]\n",
    "\n",
    "    model_name, dataset, demonstration = os.path.basename(file).split(\"_\")\n",
    "    demonstration = demonstration[:-4]\n",
    "\n",
    "    dataset_overall_demographics = overall_demographics[dataset]\n",
    "\n",
    "    responses = [x.lower() for x in responses]\n",
    "\n",
    "    conv = lambda i: i or \"\"\n",
    "    responses = [conv(i) for i in responses]\n",
    "\n",
    "    if dataset == \"bias\":\n",
    "        responses = [x.replace(\"lawyer\", \"attorney\") for x in responses]\n",
    "\n",
    "    # map labels to numbers to make it easier for sklearn calculations\n",
    "    labels_dict = labels_map[dataset]\n",
    "\n",
    "    labels_set = list(labels_dict.keys())\n",
    "\n",
    "    # map the labels lists to dummy labels\n",
    "    dummy_labels = [labels_dict[x] for x in labels]\n",
    "\n",
    "    dummy_preds = []\n",
    "\n",
    "    for pred in responses:\n",
    "        # see if any of the labels are in the response\n",
    "        for label in labels_set:\n",
    "            if pred.find(label) != -1:\n",
    "                dummy_preds.append(labels_dict[label])\n",
    "                break\n",
    "            # if not we add -1 instead\n",
    "        else:\n",
    "            dummy_preds.append(-1)\n",
    "\n",
    "    dummy_preds = np.array(dummy_preds)\n",
    "    dummy_labels = np.array(dummy_labels)\n",
    "\n",
    "    demographic_index = [\n",
    "        i\n",
    "        for i, item in enumerate(demographics)\n",
    "        if len(set(dataset_overall_demographics).intersection(set(item))) != 0\n",
    "    ]\n",
    "\n",
    "    dummy_preds = dummy_preds[demographic_index]\n",
    "    dummy_labels = dummy_labels[demographic_index]\n",
    "\n",
    "    demographics_filtered = copy.deepcopy([demographics[i] for i in demographic_index])\n",
    "\n",
    "    return dummy_preds, dummy_labels, demographics_filtered, dataset_overall_demographics, labels_dict\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KWH Over F1 Demographics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "offline-ul2 hatexplain-race 10 diversity KruskalResult(statistic=4095.353936641809, pvalue=0.0)\n",
      "offline-ul2 aae 10 similarity KruskalResult(statistic=1499.250377061469, pvalue=0.0)\n",
      "offline-ul2 bias 10 random KruskalResult(statistic=6.7752116581705195, pvalue=0.009243252894193957)\n",
      "offline-ul2 hatexplain-race 0 zeroshot KruskalResult(statistic=1554.3151540817005, pvalue=0.0)\n",
      "offline-ul2 bias 10 similarity KruskalResult(statistic=1448.125594008996, pvalue=0.0)\n",
      "offline-ul2 hatexplain-race 10 stratified KruskalResult(statistic=3896.086086337278, pvalue=0.0)\n",
      "offline-ul2 bias 0 zeroshot KruskalResult(statistic=1499.2503748125928, pvalue=0.0)\n",
      "offline-ul2 hatexplain-race 10 within KruskalResult(statistic=4079.9133408474127, pvalue=0.0)\n",
      "offline-ul2 hatexplain-race 10 random KruskalResult(statistic=4053.628325780129, pvalue=0.0)\n",
      "offline-ul2 aae 0 zeroshot KruskalResult(statistic=1491.5421671304339, pvalue=0.0)\n",
      "offline-ul2 aae 10 random KruskalResult(statistic=1491.1295403747422, pvalue=0.0)\n",
      "offline-ul2 hatexplain-race 10 similarity KruskalResult(statistic=4064.0723188869824, pvalue=0.0)\n",
      "offline-ul2 bias 10 stratified KruskalResult(statistic=154.19756042578683, pvalue=2.0967868673491052e-35)\n",
      "offline-ul2 aae 10 stratified KruskalResult(statistic=1477.2984246917881, pvalue=0.0)\n",
      "offline-ul2 bias 10 diversity KruskalResult(statistic=673.6143174482388, pvalue=1.635315526118948e-148)\n",
      "offline-ul2 aae 10 diversity KruskalResult(statistic=9.314961391086348, pvalue=0.0022729016887206763)\n",
      "offline-ul2 bias 10 within KruskalResult(statistic=1496.7027323838074, pvalue=0.0)\n",
      "offline-ul2 aae 10 within KruskalResult(statistic=1499.2504400300033, pvalue=0.0)\n",
      "chatgpt hatexplain-race 10 random KruskalResult(statistic=2300.4950802400144, pvalue=0.0)\n",
      "chatgpt bias 10 diversity KruskalResult(statistic=1263.063430530733, pvalue=1.2026665443845712e-276)\n",
      "chatgpt bias 10 within KruskalResult(statistic=1401.1091823928036, pvalue=1.2061503011751493e-306)\n",
      "chatgpt aae 0 zeroshot KruskalResult(statistic=1499.2503748125928, pvalue=0.0)\n",
      "chatgpt aae 10 random KruskalResult(statistic=1497.4458206956506, pvalue=0.0)\n",
      "chatgpt bias 10 stratified KruskalResult(statistic=1496.4271174872565, pvalue=0.0)\n",
      "chatgpt aae 10 diversity KruskalResult(statistic=1499.2383808335835, pvalue=0.0)\n",
      "chatgpt aae 10 stratified KruskalResult(statistic=1499.1964034092537, pvalue=0.0)\n",
      "chatgpt bias 10 similarity KruskalResult(statistic=1476.274612989505, pvalue=0.0)\n",
      "chatgpt hatexplain-race 10 similarity KruskalResult(statistic=2060.022529226943, pvalue=0.0)\n",
      "chatgpt aae 10 within KruskalResult(statistic=1499.2143930194889, pvalue=0.0)\n",
      "chatgpt hatexplain-race 10 stratified KruskalResult(statistic=919.1575233215848, pvalue=1.1770196642102934e-197)\n",
      "chatgpt bias 10 random KruskalResult(statistic=1496.6607893493256, pvalue=0.0)\n",
      "chatgpt aae 10 similarity KruskalResult(statistic=1498.1711088455777, pvalue=0.0)\n",
      "chatgpt hatexplain-race 10 excluding KruskalResult(statistic=1904.5605467521461, pvalue=0.0)\n",
      "chatgpt hatexplain-race 10 diversity KruskalResult(statistic=1010.8654344719367, pvalue=1.5771815420688946e-217)\n",
      "chatgpt bias 10 excluding KruskalResult(statistic=1494.2650208335826, pvalue=0.0)\n",
      "chatgpt hatexplain-race 10 within KruskalResult(statistic=252.58505927759475, pvalue=1.8057357213762558e-53)\n",
      "chatgpt hatexplain-race 0 zeroshot KruskalResult(statistic=2175.653927827986, pvalue=0.0)\n",
      "chatgpt bias 0 zeroshot KruskalResult(statistic=1473.5860438020982, pvalue=0.0)\n",
      "gpt3 aae 10 similarity KruskalResult(statistic=1499.2503748125928, pvalue=0.0)\n",
      "gpt3 hatexplain-race 10 diversity KruskalResult(statistic=1298.3940686813726, pvalue=7.419169653319661e-280)\n",
      "gpt3 hatexplain-race 10 similarity KruskalResult(statistic=1310.725880186435, pvalue=1.5726624787774212e-282)\n",
      "gpt3 bias 0 zeroshot KruskalResult(statistic=1499.2503748125928, pvalue=0.0)\n",
      "gpt3 hatexplain-race 10 stratified KruskalResult(statistic=999.549821944308, pvalue=4.468446201185599e-215)\n",
      "gpt3 bias 10 stratified KruskalResult(statistic=1499.2503748125928, pvalue=0.0)\n",
      "gpt3 aae 10 random KruskalResult(statistic=1499.1364339490256, pvalue=0.0)\n",
      "gpt3 aae 10 diversity KruskalResult(statistic=1499.2503759370309, pvalue=0.0)\n",
      "gpt3 bias 10 random KruskalResult(statistic=1499.2503748125928, pvalue=0.0)\n",
      "gpt3 aae 10 within KruskalResult(statistic=1499.1304371814094, pvalue=0.0)\n",
      "gpt3 aae 0 zeroshot KruskalResult(statistic=1496.1275648635674, pvalue=0.0)\n",
      "gpt3 aae 10 stratified KruskalResult(statistic=1498.4588751064448, pvalue=0.0)\n",
      "gpt3 hatexplain-race 0 zeroshot KruskalResult(statistic=1844.754284013933, pvalue=0.0)\n",
      "gpt3 hatexplain-race 10 excluding KruskalResult(statistic=1675.9550708583988, pvalue=0.0)\n",
      "gpt3 bias 10 excluding KruskalResult(statistic=1499.2503748125928, pvalue=0.0)\n",
      "gpt3 bias 10 diversity KruskalResult(statistic=1499.2503748125928, pvalue=0.0)\n",
      "gpt3 bias 10 similarity KruskalResult(statistic=1499.2503748125928, pvalue=0.0)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 28\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(resamples):\n\u001b[1;32m     26\u001b[0m     sample_index \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mchoice(\u001b[39mlen\u001b[39m(preds_to_bootstrap), size\u001b[39m=\u001b[39m\u001b[39mlen\u001b[39m(preds_to_bootstrap), replace\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m---> 28\u001b[0m     score \u001b[39m=\u001b[39m f1_score(\n\u001b[1;32m     29\u001b[0m         preds_to_bootstrap[sample_index],\n\u001b[1;32m     30\u001b[0m         labels_to_bootstrap[sample_index],\n\u001b[1;32m     31\u001b[0m         average\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mmacro\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     32\u001b[0m         labels\u001b[39m=\u001b[39;49m\u001b[39mlist\u001b[39;49m(labels_dict\u001b[39m.\u001b[39;49mvalues()),\n\u001b[1;32m     33\u001b[0m     )\n\u001b[1;32m     35\u001b[0m     scores\u001b[39m.\u001b[39mappend(score\u001b[39m*\u001b[39m\u001b[39m100\u001b[39m)\n\u001b[1;32m     36\u001b[0m demographic_samples\u001b[39m.\u001b[39mappend(scores)\n",
      "File \u001b[0;32m~/miniconda3/envs/fairness/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1146\u001b[0m, in \u001b[0;36mf1_score\u001b[0;34m(y_true, y_pred, labels, pos_label, average, sample_weight, zero_division)\u001b[0m\n\u001b[1;32m   1011\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mf1_score\u001b[39m(\n\u001b[1;32m   1012\u001b[0m     y_true,\n\u001b[1;32m   1013\u001b[0m     y_pred,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1019\u001b[0m     zero_division\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mwarn\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   1020\u001b[0m ):\n\u001b[1;32m   1021\u001b[0m     \u001b[39m\"\"\"Compute the F1 score, also known as balanced F-score or F-measure.\u001b[39;00m\n\u001b[1;32m   1022\u001b[0m \n\u001b[1;32m   1023\u001b[0m \u001b[39m    The F1 score can be interpreted as a harmonic mean of the precision and\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1144\u001b[0m \u001b[39m    array([0.66666667, 1.        , 0.66666667])\u001b[39;00m\n\u001b[1;32m   1145\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1146\u001b[0m     \u001b[39mreturn\u001b[39;00m fbeta_score(\n\u001b[1;32m   1147\u001b[0m         y_true,\n\u001b[1;32m   1148\u001b[0m         y_pred,\n\u001b[1;32m   1149\u001b[0m         beta\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,\n\u001b[1;32m   1150\u001b[0m         labels\u001b[39m=\u001b[39;49mlabels,\n\u001b[1;32m   1151\u001b[0m         pos_label\u001b[39m=\u001b[39;49mpos_label,\n\u001b[1;32m   1152\u001b[0m         average\u001b[39m=\u001b[39;49maverage,\n\u001b[1;32m   1153\u001b[0m         sample_weight\u001b[39m=\u001b[39;49msample_weight,\n\u001b[1;32m   1154\u001b[0m         zero_division\u001b[39m=\u001b[39;49mzero_division,\n\u001b[1;32m   1155\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/fairness/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1287\u001b[0m, in \u001b[0;36mfbeta_score\u001b[0;34m(y_true, y_pred, beta, labels, pos_label, average, sample_weight, zero_division)\u001b[0m\n\u001b[1;32m   1158\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfbeta_score\u001b[39m(\n\u001b[1;32m   1159\u001b[0m     y_true,\n\u001b[1;32m   1160\u001b[0m     y_pred,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1167\u001b[0m     zero_division\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mwarn\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   1168\u001b[0m ):\n\u001b[1;32m   1169\u001b[0m     \u001b[39m\"\"\"Compute the F-beta score.\u001b[39;00m\n\u001b[1;32m   1170\u001b[0m \n\u001b[1;32m   1171\u001b[0m \u001b[39m    The F-beta score is the weighted harmonic mean of precision and recall,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1284\u001b[0m \u001b[39m    array([0.71..., 0.        , 0.        ])\u001b[39;00m\n\u001b[1;32m   1285\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1287\u001b[0m     _, _, f, _ \u001b[39m=\u001b[39m precision_recall_fscore_support(\n\u001b[1;32m   1288\u001b[0m         y_true,\n\u001b[1;32m   1289\u001b[0m         y_pred,\n\u001b[1;32m   1290\u001b[0m         beta\u001b[39m=\u001b[39;49mbeta,\n\u001b[1;32m   1291\u001b[0m         labels\u001b[39m=\u001b[39;49mlabels,\n\u001b[1;32m   1292\u001b[0m         pos_label\u001b[39m=\u001b[39;49mpos_label,\n\u001b[1;32m   1293\u001b[0m         average\u001b[39m=\u001b[39;49maverage,\n\u001b[1;32m   1294\u001b[0m         warn_for\u001b[39m=\u001b[39;49m(\u001b[39m\"\u001b[39;49m\u001b[39mf-score\u001b[39;49m\u001b[39m\"\u001b[39;49m,),\n\u001b[1;32m   1295\u001b[0m         sample_weight\u001b[39m=\u001b[39;49msample_weight,\n\u001b[1;32m   1296\u001b[0m         zero_division\u001b[39m=\u001b[39;49mzero_division,\n\u001b[1;32m   1297\u001b[0m     )\n\u001b[1;32m   1298\u001b[0m     \u001b[39mreturn\u001b[39;00m f\n",
      "File \u001b[0;32m~/miniconda3/envs/fairness/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1577\u001b[0m, in \u001b[0;36mprecision_recall_fscore_support\u001b[0;34m(y_true, y_pred, beta, labels, pos_label, average, warn_for, sample_weight, zero_division)\u001b[0m\n\u001b[1;32m   1575\u001b[0m \u001b[39m# Calculate tp_sum, pred_sum, true_sum ###\u001b[39;00m\n\u001b[1;32m   1576\u001b[0m samplewise \u001b[39m=\u001b[39m average \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39msamples\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m-> 1577\u001b[0m MCM \u001b[39m=\u001b[39m multilabel_confusion_matrix(\n\u001b[1;32m   1578\u001b[0m     y_true,\n\u001b[1;32m   1579\u001b[0m     y_pred,\n\u001b[1;32m   1580\u001b[0m     sample_weight\u001b[39m=\u001b[39;49msample_weight,\n\u001b[1;32m   1581\u001b[0m     labels\u001b[39m=\u001b[39;49mlabels,\n\u001b[1;32m   1582\u001b[0m     samplewise\u001b[39m=\u001b[39;49msamplewise,\n\u001b[1;32m   1583\u001b[0m )\n\u001b[1;32m   1584\u001b[0m tp_sum \u001b[39m=\u001b[39m MCM[:, \u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m]\n\u001b[1;32m   1585\u001b[0m pred_sum \u001b[39m=\u001b[39m tp_sum \u001b[39m+\u001b[39m MCM[:, \u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/envs/fairness/lib/python3.9/site-packages/sklearn/metrics/_classification.py:503\u001b[0m, in \u001b[0;36mmultilabel_confusion_matrix\u001b[0;34m(y_true, y_pred, sample_weight, labels, samplewise)\u001b[0m\n\u001b[1;32m    501\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    502\u001b[0m     n_labels \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(labels)\n\u001b[0;32m--> 503\u001b[0m     labels \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mhstack(\n\u001b[1;32m    504\u001b[0m         [labels, np\u001b[39m.\u001b[39;49msetdiff1d(present_labels, labels, assume_unique\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)]\n\u001b[1;32m    505\u001b[0m     )\n\u001b[1;32m    507\u001b[0m \u001b[39mif\u001b[39;00m y_true\u001b[39m.\u001b[39mndim \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m    508\u001b[0m     \u001b[39mif\u001b[39;00m samplewise:\n",
      "File \u001b[0;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mhstack\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/fairness/lib/python3.9/site-packages/numpy/core/shape_base.py:363\u001b[0m, in \u001b[0;36mhstack\u001b[0;34m(tup, dtype, casting)\u001b[0m\n\u001b[1;32m    359\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m overrides\u001b[39m.\u001b[39mARRAY_FUNCTION_ENABLED:\n\u001b[1;32m    360\u001b[0m     \u001b[39m# raise warning if necessary\u001b[39;00m\n\u001b[1;32m    361\u001b[0m     _arrays_for_stack_dispatcher(tup, stacklevel\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\n\u001b[0;32m--> 363\u001b[0m arrs \u001b[39m=\u001b[39m atleast_1d(\u001b[39m*\u001b[39;49mtup)\n\u001b[1;32m    364\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(arrs, \u001b[39mlist\u001b[39m):\n\u001b[1;32m    365\u001b[0m     arrs \u001b[39m=\u001b[39m [arrs]\n",
      "File \u001b[0;32m<__array_function__ internals>:179\u001b[0m, in \u001b[0;36matleast_1d\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "folders = [os.path.join(data_dir, x) for x in os.listdir(data_dir)]\n",
    "\n",
    "for folder in folders:\n",
    "    files = [os.path.join(folder, x) for x in os.listdir(folder) if x.find(\".csv\") != -1]\n",
    "\n",
    "    for file in files:\n",
    "\n",
    "        dummy_preds, dummy_labels, demographics_filtered, dataset_overall_demographics, labels_dict = open_and_clean_file(file)\n",
    "\n",
    "        model_name, dataset, demonstration = os.path.basename(file).split(\"_\")\n",
    "        demonstration = demonstration[:-4]\n",
    "\n",
    "        shots = 10 if demonstration != \"zeroshot\" else 0\n",
    "\n",
    "        demographic_samples = []\n",
    "\n",
    "        for dem in dataset_overall_demographics:\n",
    "            index = [i for i, item in enumerate(demographics_filtered) if dem in item]\n",
    "\n",
    "            preds_to_bootstrap = dummy_preds[index]\n",
    "            labels_to_bootstrap = dummy_labels[index]\n",
    "\n",
    "            scores = []\n",
    "\n",
    "            for i in range(resamples):\n",
    "                sample_index = np.random.choice(len(preds_to_bootstrap), size=len(preds_to_bootstrap), replace=True)\n",
    "\n",
    "                score = f1_score(\n",
    "                    preds_to_bootstrap[sample_index],\n",
    "                    labels_to_bootstrap[sample_index],\n",
    "                    average=\"macro\",\n",
    "                    labels=list(labels_dict.values()),\n",
    "                )\n",
    "\n",
    "                scores.append(score*100)\n",
    "            demographic_samples.append(scores)\n",
    "        \n",
    "        print(model_name, dataset, shots, demonstration, kruskal(*demographic_samples))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KWH Over Demonstrations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "folders = [os.path.join(data_dir, x) for x in os.listdir(data_dir)]\n",
    "\n",
    "model_dataset_pairs = defaultdict(list)\n",
    "\n",
    "for folder in folders:\n",
    "    files = [os.path.join(folder, x) for x in os.listdir(folder) if x.find(\".csv\") != -1]\n",
    "    \n",
    "    for file in tqdm(files):\n",
    "\n",
    "        model_name, dataset, demonstration = os.path.basename(file).split(\"_\")\n",
    "\n",
    "        demonstration = demonstration[:-4]\n",
    "\n",
    "        if demonstration in [\"diversity\", \"random\", \"similarity\", \"stratified\", \"within\", \"zeroshot\"]:\n",
    "\n",
    "            dummy_preds, dummy_labels, demographics_filtered, dataset_overall_demographics, labels_dict = open_and_clean_file(file)\n",
    "\n",
    "            model_name, dataset, demonstration = os.path.basename(file).split(\"_\")\n",
    "\n",
    "            scores = []\n",
    "\n",
    "            for i in range(resamples):\n",
    "                sample_index = np.random.choice(len(preds_to_bootstrap), size=len(preds_to_bootstrap), replace=True)\n",
    "\n",
    "                score = f1_score(\n",
    "                    preds_to_bootstrap[sample_index],\n",
    "                    labels_to_bootstrap[sample_index],\n",
    "                    average=\"macro\",\n",
    "                    labels=list(labels_dict.values()),\n",
    "                )\n",
    "\n",
    "                scores.append(score*100)\n",
    "            model_dataset_pairs[(model_name, dataset)].append(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in model_dataset_pairs:\n",
    "\n",
    "    model, dataset = key\n",
    "\n",
    "    print(model, dataset, kruskal(*model_dataset_pairs[key]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KWH Over Demographic Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folders = [os.path.join(data_dir, x) for x in os.listdir(data_dir)]\n",
    "import csv\n",
    "with open(\"recall3.csv\",\"w\",) as csvfile:\n",
    "    csvwriter = csv.writer(csvfile)\n",
    "    for folder in folders:\n",
    "        files = [os.path.join(folder, x) for x in os.listdir(folder) if x.find(\".csv\") != -1]\n",
    "\n",
    "        for file in files:\n",
    "            dummy_preds, dummy_labels, demographics_filtered, dataset_overall_demographics, labels_dict = open_and_clean_file(file)\n",
    "\n",
    "            model_name, dataset, demonstration = os.path.basename(file).split(\"_\")\n",
    "            demonstration = demonstration[:-4]\n",
    "\n",
    "            shots = 10 if demonstration != \"zeroshot\" else 0\n",
    "\n",
    "            demographic_samples = []\n",
    "\n",
    "            for dem in dataset_overall_demographics:\n",
    "                index = [i for i, item in enumerate(demographics_filtered) if dem in item]\n",
    "\n",
    "                preds_to_bootstrap = dummy_preds[index]\n",
    "                labels_to_bootstrap = dummy_labels[index]\n",
    "\n",
    "                scores = []\n",
    "\n",
    "                for i in range(resamples):\n",
    "                    sample_index = np.random.choice(len(preds_to_bootstrap), size=len(preds_to_bootstrap), replace=True)\n",
    "\n",
    "                    cnf_matrix = confusion_matrix(preds_to_bootstrap[sample_index], labels_to_bootstrap[sample_index], labels=list(labels_dict.values()))\n",
    "\n",
    "                    fn = cnf_matrix.sum(axis=1) - np.diag(cnf_matrix)\n",
    "                    tp = np.diag(cnf_matrix)\n",
    "\n",
    "                    fn = fn.astype(float)\n",
    "                    tp = tp.astype(float)\n",
    "\n",
    "                    recall = tp / (tp + fn)\n",
    "\n",
    "                    if dataset == \"aae\" or dataset == \"hatexplain-race\":\n",
    "                        recall = recall[0]\n",
    "                        if np.isnan(recall):\n",
    "                            recall = 0  \n",
    "                    else:\n",
    "                        recall = np.min(recall[:-1])\n",
    "\n",
    "                        if np.isnan(recall):\n",
    "                            recall = 0  \n",
    "\n",
    "                    scores.append(recall)\n",
    "                    \n",
    "                demographic_samples.append(scores)\n",
    "            try: \n",
    "                csvwriter.writerow([model_name, dataset, shots, demonstration, kruskal(*demographic_samples)])\n",
    "            except ValueError: \n",
    "                csvwriter.writerow([model_name, dataset, shots, demonstration, \"failed\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fairness",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
